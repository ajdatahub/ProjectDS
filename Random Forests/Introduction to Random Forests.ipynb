{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Forests\n",
    "\n",
    "Decision trees are a powerful and popular machine learning technique. The basic concept is very similar to trees that we commonly use to aid decision-making.\n",
    "\n",
    "Machine Learning techniques enables us to automatically construct a decision tree that tells us what outcomes we should predict in certain situations.\n",
    "\n",
    "The decision tree algorithm is a supervised learning algorithm -- we first construct the tree with historical data, and then use it to predict an outcome. \n",
    "- One of the major advantages of decision trees is that they can pick up nonlinear interactions between variables in the data that linear regression can't.\n",
    "\n",
    "The following work is a walk through of the building blocks of making a decision tree automatically.\n",
    "\n",
    "We'll be looking at individual income in the United States. \n",
    "The data is from the 1994 census, and contains information on an individual's \n",
    "- marital status\n",
    "- age \n",
    "- type of work, and more. \n",
    "\n",
    "The target column, or what we want to predict, is whether individuals make less than or equal to 50k a year, or more than 50k a year. The dataset can be accessed from the [University of California, Irvine's website](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "\n",
    "Following projects shows the implementation and application of Decision Trees on the same data set -\n",
    "\n",
    "- [Introduction to Decision Trees](https://github.com/ajdatahub/ProjectDS/blob/master/Decision%20Trees/Introduction%20to%20Decision%20Trees.ipynb)\n",
    "\n",
    "- [Applying Decision Trees](https://github.com/ajdatahub/ProjectDS/blob/master/Decision%20Trees/Applying%20Decision%20Trees.ipynb)\n",
    "\n",
    "In these projects, I have learned about how decision trees are constructed and how to use them most effectively.\n",
    "\n",
    "\n",
    "The most powerful tool for reducing decision tree overfitting is called the random forest algorithm. In the current project, we'll learn how to construct and apply Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education_num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital_status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
      "0          2174             0              40   United-States       <=50K  \n",
      "1             0             0              13   United-States       <=50K  \n",
      "2             0             0              40   United-States       <=50K  \n",
      "3             0             0              40   United-States       <=50K  \n",
      "4             0             0              40            Cuba       <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First column is age of the person. Set index_col to False to avoid pandas thinking that the first column is row indexes\n",
    "income = pd.read_csv(\"income.csv\", index_col=False)\n",
    "print(income.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have categorical variables such as workclass that have string values. \n",
    "- Multiple individuals can share the same string value. The types of work include State-gov, Self-emp-not-inc, Private, and so on. Each of these strings is a label for a category. \n",
    "- Another example of a column of categories is sex, where the options are Male and Female.\n",
    "\n",
    "Before we get started with decision trees, we need to convert the categorical variables in our data set to numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7\n",
       "1    6\n",
       "2    4\n",
       "3    4\n",
       "4    4\n",
       "Name: workclass, dtype: int8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workclass_col = pd.Categorical(income['workclass'])\n",
    "income['workclass'] = workclass_col.codes\n",
    "income['workclass'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will convert all the other categorical variables to numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical(column,data):\n",
    "#     for each in columns:\n",
    "    column_cat = pd.Categorical(data[column])\n",
    "    codes = column_cat.codes\n",
    "    return codes\n",
    "    \n",
    "income['education'] = convert_categorical('education',income)\n",
    "income['marital_status'] = convert_categorical('marital_status',income)\n",
    "income['occupation'] = convert_categorical('occupation',income)\n",
    "income['relationship'] = convert_categorical('relationship',income)\n",
    "income['race'] = convert_categorical('race',income)\n",
    "income['sex'] = convert_categorical('sex',income)\n",
    "income['native_country'] = convert_categorical('native_country',income)\n",
    "income['high_income'] = convert_categorical('high_income',income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is made up of a series of nodes and branches. A node is where we split the data based on a variable, and a branch is one side of the split. \n",
    "\n",
    "The nodes at the bottom of the tree, where we decide to stop splitting, are called terminal nodes, or leaves.\n",
    "- When we are splitting the data, we are not doing it randomly. We have a motive in mind- an objective. Out goal is to make predictions on future data. \n",
    "    - In order to do this, all rows in each leaf must have only one value for our target column.\n",
    "    \n",
    "Each leaf can only have rows with the same values for our target column. If this isn't the case, we won't be able to make effective predictions.\n",
    "\n",
    "After constructing a tree using the data, we'll want to make predictions. In order to do this, we'll take a new row and feed it through our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
       "0   39          7   77516          9             13               4   \n",
       "1   50          6   83311          9             13               2   \n",
       "2   38          4  215646         11              9               0   \n",
       "3   53          4  234721          1              7               2   \n",
       "4   28          4  338409          9             13               2   \n",
       "\n",
       "   occupation  relationship  race  sex  capital_gain  capital_loss  \\\n",
       "0           1             1     4    1          2174             0   \n",
       "1           4             0     4    1             0             0   \n",
       "2           6             1     4    1             0             0   \n",
       "3           6             0     2    1             0             0   \n",
       "4          10             5     2    0             0             0   \n",
       "\n",
       "   hours_per_week  native_country  high_income  \n",
       "0              40              39            0  \n",
       "1              13              39            0  \n",
       "2              40              39            0  \n",
       "3              40              39            0  \n",
       "4              40               5            0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "A random forest is a kind of ensemble model. Ensembles combine the predictions of multiple models to create a more accurate final prediction. We'll make a simple ensemble to see how they work.\n",
    "\n",
    "To learn about this, let us create two decision trees with slightly different parameters:\n",
    "\n",
    "- One with min_samples_leaf set to 2\n",
    "- One with max_depth set to 5\n",
    "\n",
    "We will check their accuracies separately. Lter, we'll combine their predictions and compare the combined accuracy with the individual accuracies of both trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set a random seed so the shuffle is the same every time\n",
    "np.random.seed(1)\n",
    "\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "split_val = math.floor(income.shape[0] * .8)\n",
    "\n",
    "train = income.iloc[:split_val]\n",
    "test = income.iloc[split_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "cl = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "cl.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "cl1 = DecisionTreeClassifier(random_state=1, max_depth=5)\n",
    "cl1.fit(train[columns], train[\"high_income\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use AUC_ROC( Area Under the Receiver Operating Characteristic curve) as an error metric. AUC ranges from 0 to 1, so it's ideal for binary classification. The higher the AUC, the more accurate our predictions.\n",
    "\n",
    "We can compute AUC with the roc_auc_score function from sklearn.metrics. This function takes in two parameters:\n",
    "- y_true: true labels\n",
    "- y_score: predicted labels\n",
    "\n",
    "It then calculates and returns the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Samples AUC :  0.6878964226062301\n",
      "Max Depth AUC :  0.6759853906508785\n"
     ]
    }
   ],
   "source": [
    "# min_samples_leaf=2 predictions\n",
    "min_samples_predictions = cl.predict(test[columns])\n",
    "\n",
    "min_samples_auc = roc_auc_score(test['high_income'],min_samples_predictions)\n",
    "\n",
    "# max_depth=5 predictions\n",
    "max_depth_predictions = cl1.predict(test[columns])\n",
    "\n",
    "max_depth_auc = roc_auc_score(test['high_income'], max_depth_predictions)\n",
    "\n",
    "print('Min Samples AUC : ',min_samples_auc)\n",
    "print('Max Depth AUC : ',max_depth_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Predictions\n",
    "\n",
    "When we have multiple classifiers making predictions, we can treat each set of predictions as a column in a matrix. Here's an example where we have Decision Tree 1 (DT1), Decision Tree 2 (DT2), and Decision Tree 3 (DT3):\n",
    "\n",
    "DT1     DT2    DT3 <br>\n",
    " 0       1      0  <br>\n",
    " 1       1      1 <br>\n",
    " 0       0      1 <br>\n",
    " 1       0      0 <br>\n",
    "\n",
    "But, we want a single vector containing one prediction per row in the training data set. \n",
    "To accomplish this, we'll need to create rules to convert each row of our matrix of predictions into a single number.\n",
    "\n",
    "We want to create a Final Prediction vector that looks like this:\n",
    "\n",
    "DT1 &nbsp;            DT2 &nbsp;           DT3 &nbsp;           Final Prediction<br>\n",
    "0 &nbsp;              1 &nbsp;            0 &nbsp;            0<br>\n",
    "1       1      1      1<br>\n",
    "0       0      1      0<br>\n",
    "1       0      0      0<br>\n",
    "\n",
    "There are many ways to get from the output of multiple models to a final vector of predictions. \n",
    "- One method is majority voting, in which each classifier gets a \"vote,\" and the most commonly voted value for each row \"wins.\" \n",
    "- This only works if there are more than two classifiers (and ideally an odd number, so we don't have to write a rule to break ties). Majority voting is what we applied in the example above.\n",
    "\n",
    "Before, we just used 2 classifiers (with max_depth and min_samples_leaf). As we have two classifiers, we'll take the mean of all of the items in a row. \n",
    "\n",
    "- Instead of using the predict() method, which returns either 0 or 1 we can use the RandomForestClassifier.predict_proba() method instead, which will predict a probability from 0 to 1 that a given class is the right one for a row. \n",
    "- predict_proba() will return the following output- \n",
    "\n",
    " 0     1<br>\n",
    ".7    .3<br>\n",
    ".2    .8<br>\n",
    ".1    .9<br>\n",
    "\n",
    "\n",
    "- Each row will correspond to a prediction. The first column is the probability that the prediction is a 0, and the second column is the probability that the prediction is a 1. Each row adds up to 1.\n",
    "\n",
    "\n",
    "- If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector, instead of just 0 or 1.\n",
    "\n",
    "\n",
    "- Then we can add together all of the vectors we get through this method, and divide the sum by the total number of vectors to get the mean prediction made across the entire ensemble for a particular row. Finally, we round off to get a 0 or 1 prediction for the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined AUC:  0.7150846804038882\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "If we use the predict_proba() method on both classifiers to generate probabilities, \n",
    "take the mean for each row, and then round the results, we'll get ensemble predictions.\n",
    "'''\n",
    "\n",
    "predictions1 = cl.predict_proba(test[columns])[:,1]\n",
    "predictions2 = cl1.predict_proba(test[columns])[:,1]\n",
    "\n",
    "mean_predictions = np.round((predictions1 + predictions2)/2)\n",
    "\n",
    "combined_auc_score = roc_auc_score(test['high_income'],mean_predictions)\n",
    "\n",
    "print('Combined AUC: ',combined_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the AUC value for the combined predictions of the two trees is higher than that of either tree on its own.\n",
    "\n",
    "The individual models are approaching the same problem in slightly different ways, and building different trees because we used different parameters for each one. \n",
    "- Each tree makes different predictions in different areas. \n",
    "- Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches.\n",
    "\n",
    "\n",
    "- The more \"diverse\" or dissimilar the models we use to construct an ensemble are, the stronger their combined predictions will be (assuming that all of the models have about the same accuracy). \n",
    "\n",
    "\n",
    "- Ensembling a decision tree and a logistic regression model, for example, will result in stronger predictions than ensembling two decision trees with similar parameters. That's because those two models use very different approaches to arrive at their answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation with Bagging\n",
    "\n",
    "Random Forests is an ensemble of Decision Trees. \n",
    "\n",
    "\n",
    "- If we don't make any modifications to the trees, each tree will be exactly the same, so we'll get no boost when we ensemble them. In order to make ensembling effective, we have to introduce variation into each individual decision tree model.\n",
    "\n",
    "If we introduce variation, each tree will be be constructed slightly differently, and will therefore make different predictions.\n",
    "\n",
    "- There are two main ways to introduce variation in a random forest -bagging and random feature subsets. \n",
    "\n",
    "\n",
    "__Bagging- __\n",
    "\n",
    "In a random forest, we don't train each tree on the entire data set. We train it on a random sample of the data, or a \"bag,\" instead. We perform this sampling with replacement, which means that after we select a row from the data we're sampling, we put the row back in the data so it can be picked again. Some rows from the original data may appear in the \"bag\" multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined AUC:  0.7329963297474371\n"
     ]
    }
   ],
   "source": [
    "tree_count = 10 # Number of trees we want to build\n",
    "\n",
    "# List to hold predicitons for each tree\n",
    "predictions_list = [] \n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "for i in range(tree_count):\n",
    "    \n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    cl = DecisionTreeClassifier(random_state=1, min_samples_leaf=2)\n",
    "    cl.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions_list.append(cl.predict_proba(test[columns])[:,1])\n",
    "    \n",
    "mean_predictions = [np.round(sum(i)/tree_count) for i in zip(*predictions_list)]\n",
    "\n",
    "combined_auc_score = roc_auc_score(test['high_income'],mean_predictions)\n",
    "\n",
    "print('Combined AUC: ',combined_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bagging, we gained some accuracy over a single decision tree. \n",
    "- We achieved an AUC score of around .733 with bagging, which is an improvement over the AUC score of .688 we got without bagging.\n",
    "\n",
    "### Selecting Random Features\n",
    "\n",
    "To build a tree, we split the data based on a certain feature. The feature which is used to split the data is absed in the information gain that particular feature provides.  \n",
    "- We compute the information gain for each feature in our random sample, and pick the one with the highest information gain to split on.\n",
    "    - Entropy is the measure of \"disorder\" in the data set. \n",
    "    - If a dataset has all the same labels, they'll have low entropy. If all the labels are different, they'll have high entropy. \n",
    "    - Splits that give us more information about the data, will ideally minimize entropy. \n",
    "    - The tree we are building will split the labels into distinct groups with minimum disorder (mix of values). This'll allow the splits to give our tree more predictive power.\n",
    "\n",
    "\n",
    "While training our model in every iteration, rather than using all the features to find the maximum information gain, we'll only evaluate a __constrained set of features that we select randomly.__\n",
    "- This introduces variation into the trees, and makes for more powerful ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform random subset selection process in scikit-learn- \n",
    "- Set the splitter parameter on DecisionTreeClassifier to \"random\", and the max_features parameter to \"auto\". \n",
    "- If we have N columns, this will pick a subset of features of size square root of N, compute the Gini coefficient for each (this is similar to information gain), and split the node on the best column in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined AUC:  0.7345958637997538\n"
     ]
    }
   ],
   "source": [
    "tree_count = 10 # Number of trees we want to build\n",
    "\n",
    "# List to hold predicitons for each tree\n",
    "predictions_list = [] \n",
    "\n",
    "# Each \"bag\" will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "for i in range(tree_count):\n",
    "    \n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\"\n",
    "    cl = DecisionTreeClassifier(random_state=1, min_samples_leaf=2, splitter = 'random', max_features = 'auto')\n",
    "    cl.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data\n",
    "    predictions_list.append(cl.predict_proba(test[columns])[:,1])\n",
    "    \n",
    "mean_predictions = [np.round(sum(i)/tree_count) for i in zip(*predictions_list)]\n",
    "\n",
    "combined_auc_score = roc_auc_score(test['high_income'],mean_predictions)\n",
    "\n",
    "print('Combined AUC: ',combined_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, Random Forests have the two building blocks __bagging__ and __random feature subset__. \n",
    "\n",
    "Scikit-Learn has a __RandomForestClassifier__ class and a __RandomForestRegressor__ class that enable us to train and test random forest models quickly.\n",
    "\n",
    "When we instantiate a RandomForestClassifier, we pass in an n_estimators parameter that indicates how many trees to build. \n",
    "- While adding more trees usually improves accuracy, it also increases the overall time the model takes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF AUC Score 0.7347461391939776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clr = RandomForestClassifier(n_estimators = 5, random_state = 1, min_samples_leaf = 2)\n",
    "\n",
    "# Fit the model\n",
    "clr.fit(train[columns],train[['high_income']])\n",
    "\n",
    "# Make predictions\n",
    "predictions = clr.predict(test[columns])\n",
    "\n",
    "auc_score = roc_auc_score(test['high_income'], predictions)\n",
    "\n",
    "print('RF AUC Score', auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to decision trees, we can tweak some of the parameters for random forests, including:\n",
    "\n",
    "- min_samples_leaf\n",
    "- min_samples_split\n",
    "- max_depth\n",
    "- max_leaf_nodes\n",
    "\n",
    "There are also parameters specific to the random forest that alter the overall construction of the tree- \n",
    "\n",
    "- n_estimators\n",
    "- bootstrap - \"Bootstrap aggregation\" is another name for bagging; this parameter indicates whether to turn it on (Defaults to True)\n",
    "\n",
    "Tweaking parameters can increase the accuracy of the forest. \n",
    "- The easiest tweak is to increase the number of estimators we use. \n",
    "- This approach yields diminishing returns - going from 10 trees to 100 will make a bigger difference than going from 100 to 500, which will make a bigger difference than going from 500 to 1000. \n",
    "- The accuracy increase function is logarithmic, so increasing the number of trees beyond a certain number (usually 200) won't help much at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF AUC Score 0.7379403213124711\n"
     ]
    }
   ],
   "source": [
    "# Increasing n_estimators to 150.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clr = RandomForestClassifier(n_estimators = 150, random_state = 1, min_samples_leaf = 2)\n",
    "\n",
    "# Fit the model\n",
    "clr.fit(train[columns],train[['high_income']])\n",
    "\n",
    "# Make predictions\n",
    "predictions = clr.predict(test[columns])\n",
    "\n",
    "auc_score = roc_auc_score(test['high_income'], predictions)\n",
    "\n",
    "print('RF AUC Score', auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could increase the AUC score from 0.734 to 0.737 but the model took longer time to run. \n",
    "- Working with much larger data sets will affect the extra time and could amount to hours or days.\n",
    "\n",
    "One of the major advantages of random forests over single decision trees is that they tend to overfit less. \n",
    "- Although each individual decision tree in a random forest varies widely, the average of their predictions is less sensitive to the input data than a single tree is. \n",
    "- This is because while one tree can construct an incorrect and overfit model, the average of 100 or more trees will be more likely to refine the signal and ignore the noise. \n",
    "- The signal will be the same across all of the trees, whereas each tree will refine in on the noise differently. This means that the average will discard the noise and keep the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where Random Forests are Useful\n",
    "\n",
    "Random forest algorithm is incredibly powerful but it isn't applicable to all tasks. \n",
    "\n",
    "- The main strengths of a random forest are- \n",
    "    - __Very accurate predictions__ - Random forests achieve near very accurate performance on many machine learning tasks. Along with neural networks and gradient-boosted trees, they're typically one of the top-performing algorithms.\n",
    "\n",
    "    - __Resistance to overfitting__ - Due to their construction, random forests are fairly resistant to overfitting. \n",
    "    \n",
    "\n",
    "- The main weaknesses of using a random forest are:\n",
    "\n",
    "    - __Difficult to interpret__ - Because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "\n",
    "    - __Takes longer to create__ - It takes longer time to build and implement using the RandomForests algorithms. Fortunately, we can exploit multicore processors to parallelize tree construction. Scikit allows us to do this through the n_jobs parameter on RandomForestClassifier. \n",
    "\n",
    "\n",
    "Given these trade-offs, it makes sense to use random forests in situations where __accuracy is of the utmost importance__; being able to interpret or explain the decisions the model is making isn't key. \n",
    "- In cases where time is of the essence or interpretability is important, a single decision tree may be a better choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
