{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Decision Trees\n",
    "\n",
    "Decision trees are a powerful and popular machine learning technique. The basic concept is very similar to trees that we commonly use to aid decision-making.\n",
    "\n",
    "Machine Learning techniques enables us to automatically construct a decision tree that tells us what outcomes we should predict in certain situations.\n",
    "\n",
    "The decision tree algorithm is a supervised learning algorithm -- we first construct the tree with historical data, and then use it to predict an outcome. \n",
    "- One of the major advantages of decision trees is that they can pick up nonlinear interactions between variables in the data that linear regression can't.\n",
    "\n",
    "The following work is a walk through of the building blocks of making a decision tree automatically.\n",
    "\n",
    "We'll be looking at individual income in the United States. \n",
    "The data is from the 1994 census, and contains information on an individual's \n",
    "- marital status\n",
    "- age \n",
    "- type of work, and more. \n",
    "\n",
    "The target column, or what we want to predict, is whether individuals make less than or equal to 50k a year, or more than 50k a year. The dataset can be accessed from the [University of California, Irvine's website](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "An [Introduction to Decision Trees](https://github.com/ajdatahub/ProjectDS/tree/master/Decision%20Trees) and how they are implemented using the same data set can be found [here](https://github.com/ajdatahub/ProjectDS/tree/master/Decision%20Trees). \n",
    "In this project, I have learned about how decision trees are constructed. The project demonstrates a modified version of ID3, which is a bit simpler than the most common tree building algorithms, C4.5 and CART. The basics are the same, however, so we can apply what we learned about how decision trees work to any tree construction algorithm.\n",
    "\n",
    "In the current project, we'll learn about when to use decision trees, and how to use them most effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education_num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital_status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
      "0          2174             0              40   United-States       <=50K  \n",
      "1             0             0              13   United-States       <=50K  \n",
      "2             0             0              40   United-States       <=50K  \n",
      "3             0             0              40   United-States       <=50K  \n",
      "4             0             0              40            Cuba       <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First column is age of the person. Set index_col to False to avoid pandas thinking that the first column is row indexes\n",
    "income = pd.read_csv(\"income.csv\", index_col=False)\n",
    "print(income.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have categorical variables such as workclass that have string values.\n",
    "- Multiple individuals can share the same string value. The types of work include State-gov, Self-emp-not-inc, Private, and so on. Each of these strings is a label for a category.\n",
    "- Another example of a column of categories is sex, where the options are Male and Female.\n",
    "\n",
    "Before we get started with decision trees, we need to convert the categorical variables in our data set to numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7\n",
       "1    6\n",
       "2    4\n",
       "3    4\n",
       "4    4\n",
       "Name: workclass, dtype: int8"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workclass_col = pd.Categorical(income['workclass'])\n",
    "income['workclass'] = workclass_col.codes\n",
    "income['workclass'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical(column,data):\n",
    "#     for each in columns:\n",
    "    column_cat = pd.Categorical(data[column])\n",
    "    codes = column_cat.codes\n",
    "    return codes\n",
    "    \n",
    "income['education'] = convert_categorical('education',income)\n",
    "income['marital_status'] = convert_categorical('marital_status',income)\n",
    "income['occupation'] = convert_categorical('occupation',income)\n",
    "income['relationship'] = convert_categorical('relationship',income)\n",
    "income['race'] = convert_categorical('race',income)\n",
    "income['sex'] = convert_categorical('sex',income)\n",
    "income['native_country'] = convert_categorical('native_country',income)\n",
    "income['high_income'] = convert_categorical('high_income',income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>high_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
       "0   39          7   77516          9             13               4   \n",
       "1   50          6   83311          9             13               2   \n",
       "2   38          4  215646         11              9               0   \n",
       "3   53          4  234721          1              7               2   \n",
       "4   28          4  338409          9             13               2   \n",
       "\n",
       "   occupation  relationship  race  sex  capital_gain  capital_loss  \\\n",
       "0           1             1     4    1          2174             0   \n",
       "1           4             0     4    1             0             0   \n",
       "2           6             1     4    1             0             0   \n",
       "3           6             0     2    1             0             0   \n",
       "4          10             5     2    0             0             0   \n",
       "\n",
       "   hours_per_week  native_country  high_income  \n",
       "0              40              39            0  \n",
       "1              13              39            0  \n",
       "2              40              39            0  \n",
       "3              40              39            0  \n",
       "4              40               5            0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use scikit-learn package to fit a decision tree. \n",
    "- For classification problem, we can use DecisionTreeClassifier\n",
    "- For regression problem, we can use DecisionTreeRegressor\n",
    "\n",
    "In our project, we are trying to predict a binary outcome for the high-income. So, we will be using the regressor class.\n",
    "\n",
    "Firstly, we will train our classifier on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "cl = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Training the model\n",
    "cl.fit(income[columns],income[['high_income']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will split our data into train and test dataset. If we do not split our data, we will overfit our model by making prediciotns about the data which has already been feeded into the model. \n",
    "\n",
    "- This will reduce the error metric as the data has already been trained on the observations we are trying to predict.\n",
    "- We can avoid overfitting by making predictions and evaluating the error metric on the data we have not trained our algorithm or model on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set a random seed so the shuffle is the same every time\n",
    "np.random.seed(1)\n",
    "\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "split_val = math.floor(income.shape[0] * .8)\n",
    "\n",
    "train = income.iloc[:split_val]\n",
    "test = income.iloc[split_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC ranges from 0 to 1, so it's ideal for binary classification. The higher the AUC, the more accurate our predictions.\n",
    "\n",
    "We can compute AUC with the roc_auc_score function from sklearn.metrics. This function takes in two parameters:\n",
    "- y_true: true labels\n",
    "- y_score: predicted labels\n",
    "\n",
    "It then calculates and returns the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934656324746192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cl = DecisionTreeClassifier(random_state=1)\n",
    "cl.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = cl.predict(test[columns])\n",
    "\n",
    "error = roc_auc_score(test[\"high_income\"], predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this against the AUC for predictions on the training set to see if the model is overfitting.\n",
    "\n",
    "- It's normal for the model to predict the training set better than the testing set. After all, it has full knowledge of that data and the outcomes. \n",
    "- However, if the AUC between training set predictions and actual values is significantly higher than the AUC between test set predictions and actual values, it's a sign that the model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9471244501437455\n"
     ]
    }
   ],
   "source": [
    "# Calculating predictions for train data set\n",
    "predictions1 = cl.predict(train[columns])\n",
    "\n",
    "# Calculating the roc score\n",
    "error = roc_auc_score(train[['high_income']],predictions1)\n",
    "\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AUC on the training set was .947, and the AUC on the test set was .694. \n",
    "- There's no hard and fast rule on when overfitting is occurring, but our model is predicting the training set much better than the test set. \n",
    "- Splitting the data into training and testing sets doesn't prevent overfitting -- it just helps us detect and fix it.\n",
    "\n",
    "Based on our AUC measurements, it appears that we are in fact overfitting.\n",
    "\n",
    "Trees overfit when they have too much depth and make overly complex rules that match the training data, but aren't able to generalize well to new data. \n",
    "- This may seem to be a strange principle at first, but the deeper a tree is, the worse it typically performs on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reducing Overfitting With a Shallower Tree\n",
    "\n",
    "There are three main ways to combat overfitting:\n",
    "- \"Prune\" the tree after we build it to remove unnecessary leaves.\n",
    "- Use ensembling to blend the predictions of many trees.\n",
    "- Restrict the depth of the tree while we're building it.\n",
    "\n",
    "Lets discuss the third method first.\n",
    "\n",
    "Limiting tree depth during the building process will result in more general rules. This prevents the tree from overfitting.\n",
    "\n",
    "We can restrict tree depth by adding a few parameters when we initialize the DecisionTreeClassifier class:\n",
    "\n",
    "- max_depth - Globally restricts how deep the tree can go\n",
    "- min_samples_split - The minimum number of rows a node should have before it can be split; if this is set to 2, for example, then nodes with 2 rows won't be split, and will become leaves instead\n",
    "- min_samples_leaf - The minimum number of rows a leaf must have\n",
    "- min_weight_fraction_leaf - The fraction of input rows a leaf must have\n",
    "- max_leaf_nodes - The maximum number of total leaves; this will cap the count of leaf nodes as the tree is being built\n",
    "\n",
    "Some of these parameters aren't compatible, however. For example, we can't use max_depth and max_leaf_nodes together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC :  0.6995617145150872\n",
      "Train AUC :  0.8421431849275413\n"
     ]
    }
   ],
   "source": [
    "# Setting min_samples_split to 13 when creating the DecisionTreeClassifier\n",
    "\n",
    "cl = DecisionTreeClassifier(random_state=1, min_samples_split = 13)\n",
    "cl.fit(train[columns],train['high_income'])\n",
    "\n",
    "# Predicting the labes for test data set\n",
    "predictions = cl.predict(test[columns])\n",
    "test_auc = roc_auc_score(test['high_income'],predictions)\n",
    "\n",
    "# Predicting the labes for train data set\n",
    "predictions1 = cl.predict(train[columns])\n",
    "train_auc = roc_auc_score(train['high_income'],predictions1)\n",
    "\n",
    "print('Test AUC : ',test_auc)\n",
    "print('Train AUC : ',train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting min_samples_split to 13, \n",
    "- we managed to boost the test AUC from .694 to .700. \n",
    "- the training set AUC decreased from .947 to .843\n",
    "- showing that the model we built was less overfit to the training set than before.\n",
    "\n",
    "Lets add more parameters to see how the error metric changes.(Set max_depth to 7 and min_samples_split to 13 when creating the DecisionTreeClassifier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC :  0.7436344996725136\n",
      "Train AUC :  0.748037708309209\n"
     ]
    }
   ],
   "source": [
    "# Set max_depth to 7 and min_samples_split to 13 when creating the DecisionTreeClassifier.\n",
    "\n",
    "cl = DecisionTreeClassifier(random_state = 1, min_samples_split = 13, max_depth = 7)\n",
    "cl.fit(train[columns],train['high_income'])\n",
    "\n",
    "# Predicting the labes for test data set\n",
    "predictions = cl.predict(test[columns])\n",
    "test_auc = roc_auc_score(test['high_income'],predictions)\n",
    "\n",
    "# Predicting the labes for train data set\n",
    "predictions1 = cl.predict(train[columns])\n",
    "train_auc = roc_auc_score(train['high_income'],predictions1)\n",
    "\n",
    "print('Test AUC : ',test_auc)\n",
    "print('Train AUC : ',train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't overfitting anymore because both AUC values are about the same. \n",
    "- Let's tweak the parameters more aggressively and see how the error metric changes (Set max_depth to 2 and min_samples_split to 100 when creating the DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC :  0.6553138481876499\n",
      "Train AUC :  0.6624508042161483\n"
     ]
    }
   ],
   "source": [
    "# Set max_depth to 2 and min_samples_split to 100 when creating the DecisionTreeClassifier.\n",
    "\n",
    "cl = DecisionTreeClassifier(random_state = 1, min_samples_split = 100, max_depth = 2)\n",
    "cl.fit(train[columns],train['high_income'])\n",
    "\n",
    "# Predicting the labes for test data set\n",
    "predictions = cl.predict(test[columns])\n",
    "test_auc = roc_auc_score(test['high_income'],predictions)\n",
    "\n",
    "# Predicting the labes for train data set\n",
    "predictions1 = cl.predict(train[columns])\n",
    "train_auc = roc_auc_score(train['high_income'],predictions1)\n",
    "\n",
    "print('Test AUC : ',test_auc)\n",
    "print('Train AUC : ',train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the accuracy of our model came down. This happened because we are 'Underfitting'.  \n",
    "- Underfitting is what occurs when our model is too simple to explain the relationships between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The Bias-Variance Tradeoff\n",
    "\n",
    "- By artificially restricting the depth of our tree, we prevent it from creating a model that's complex enough to correctly categorize some of the rows. \n",
    "- If we don't perform the artificial restrictions, however, the tree becomes too complex, fits quirks in the data that only exist in the training set, and doesn't generalize to new data.\n",
    "\n",
    "This is known as the bias-variance tradeoff.\n",
    "- If we take a random sample of training data and create many models. If the models' predictions for the same row are far apart from each other, we have high variance. \n",
    "- If we take a random sample of the training data and create many models. If the models' predictions for the same row are close together but far from the actual value, then we have high bias.\n",
    "\n",
    "High bias can cause underfitting. If a model is consistently failing to predict the correct value, it may be that it's too simple to model the data faithfully.\n",
    "\n",
    "High variance can cause overfitting. If a model varies its predictions significantly based on small changes in the input data, then it's likely fitting itself to quirks in the training data, rather than making a generalizable model.\n",
    "\n",
    "- Decreasing one of these properties will increase the other. The bias-variance tradeoff is a limitation of all machine learning algorithms. \n",
    "\n",
    "    - Decision trees typically suffer from high variance. \n",
    "    - The entire structure of a decision tree can change if we make a minor alteration to its training data. \n",
    "    - By restricting the depth of the tree, we increase the bias and decrease the variance. \n",
    "    - If we restrict the depth too much, we increase bias to the point where it will underfit.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "One way to prevent overfitting is to Pruning. Basically, we build the full tree and then remove (prune) off the leaves which do not add to the prediction accuracy.\n",
    "\n",
    "- Pruning prevents a model from becoming overly complex. It can result in a simpler model that has higher accuracy on the testing set.\n",
    "\n",
    "### When to use Decision Trees\n",
    "\n",
    "So, advantages of using a tree-\n",
    "- easy to interpret\n",
    "- relatively fast to fit and make predictions\n",
    "- able to pick up nonlinearities in data, and usually fairly accurate\n",
    "\n",
    "The main disadvantage of using decision trees is their tendency to overfit.\n",
    "\n",
    "- Decision trees are a good choice for tasks where it's important to be able to interpret and convey why the algorithm is doing what it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
